{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Val/Test: (6726, 12) (4068, 12) (2302, 12)\n",
      "Churn rate (train/val/test): 0.0617 0.1084 0.1486\n"
     ]
    }
   ],
   "source": [
    "# Load Feature Data + Split\n",
    "df = pd.read_csv(\"../data/processed/account_features_v1.csv\", parse_dates=[\"snapshot_date\"])\n",
    "\n",
    "TRAIN_END = \"2024-03-31\"\n",
    "VAL_END   = \"2024-08-31\"\n",
    "\n",
    "train = df[df[\"snapshot_date\"] <= TRAIN_END].copy()\n",
    "val   = df[(df[\"snapshot_date\"] > TRAIN_END) & (df[\"snapshot_date\"] <= VAL_END)].copy()\n",
    "test  = df[df[\"snapshot_date\"] > VAL_END].copy()\n",
    "\n",
    "TARGET = \"churn_next_30d\"\n",
    "FEATURES = [\n",
    "    \"usage_events_30d\",\n",
    "    \"usage_events_90d\",\n",
    "    \"usage_trend_30d\",\n",
    "    \"usage_per_seat_30d\",\n",
    "    \"tickets_30d\",\n",
    "    \"escalations_90d\",\n",
    "    \"ticket_rate_30d\",\n",
    "    \"recent_upgrade_flag\",\n",
    "    \"recent_downgrade_flag\",\n",
    "    \"seat_change_30d\",\n",
    "    \"tenure_days\",\n",
    "    \"no_active_subscription_flag\",\n",
    "]\n",
    "\n",
    "X_train, y_train = train[FEATURES], train[TARGET]\n",
    "X_val, y_val     = val[FEATURES], val[TARGET]\n",
    "X_test, y_test   = test[FEATURES], test[TARGET]\n",
    "\n",
    "print(\"Train/Val/Test:\", X_train.shape, X_val.shape, X_test.shape)\n",
    "print(\"Churn rate (train/val/test):\", round(y_train.mean(),4), round(y_val.mean(),4), round(y_test.mean(),4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'best_model_id': 'lr_bal_C=5.0',\n",
       "  'best_params': {'C': 5.0, 'class_weight': 'balanced'},\n",
       "  'val_metrics': {'roc_auc': 0.6128744669451275, 'pr_auc': 0.1589462560880725},\n",
       "  'features': ['usage_events_30d',\n",
       "   'usage_events_90d',\n",
       "   'usage_trend_30d',\n",
       "   'usage_per_seat_30d',\n",
       "   'tickets_30d',\n",
       "   'escalations_90d',\n",
       "   'ticket_rate_30d',\n",
       "   'recent_upgrade_flag',\n",
       "   'recent_downgrade_flag',\n",
       "   'seat_change_30d',\n",
       "   'tenure_days',\n",
       "   'no_active_subscription_flag'],\n",
       "  'train_end': '2024-03-31',\n",
       "  'val_end': '2024-08-31',\n",
       "  'model_family': 'logistic_regression'},\n",
       " {'best_model_id': 'hgb_1',\n",
       "  'best_params': {'learning_rate': 0.05, 'max_depth': 5, 'max_leaf_nodes': 31},\n",
       "  'val_metrics': {'roc_auc': 0.5856923414527101,\n",
       "   'pr_auc': 0.13083983773041796},\n",
       "  'features': ['usage_events_30d',\n",
       "   'usage_events_90d',\n",
       "   'usage_trend_30d',\n",
       "   'usage_per_seat_30d',\n",
       "   'tickets_30d',\n",
       "   'escalations_90d',\n",
       "   'ticket_rate_30d',\n",
       "   'recent_upgrade_flag',\n",
       "   'recent_downgrade_flag',\n",
       "   'seat_change_30d',\n",
       "   'tenure_days',\n",
       "   'no_active_subscription_flag'],\n",
       "  'train_end': '2024-03-31',\n",
       "  'val_end': '2024-08-31',\n",
       "  'model_family': 'hist_gradient_boosting'})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Saved Models + Metadata\n",
    "best_lr = joblib.load(\"../models/best_lr_pipeline.joblib\")\n",
    "best_hgb = joblib.load(\"../models/best_hgb_pipeline.joblib\")\n",
    "\n",
    "with open(\"../models/best_lr_model_meta.json\", \"r\") as f:\n",
    "    lr_meta = json.load(f)\n",
    "\n",
    "with open(\"../models/best_hgb_model_meta.json\", \"r\") as f:\n",
    "    hgb_meta = json.load(f)\n",
    "\n",
    "lr_meta, hgb_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Evaluation Function (Ranking + Top-K)\n",
    "\n",
    "def eval_ranked_model(pipeline, X, y, label, top_pct=0.20):\n",
    "    probs = pipeline.predict_proba(X)[:, 1]\n",
    "\n",
    "    roc = roc_auc_score(y, probs)\n",
    "    pr = average_precision_score(y, probs)\n",
    "\n",
    "    # Outreach threshold: top (top_pct) risk\n",
    "    thr = np.quantile(probs, 1 - top_pct)\n",
    "    preds = (probs >= thr).astype(int)\n",
    "\n",
    "    cm = confusion_matrix(y, preds)\n",
    "    report_dict = classification_report(y, preds, digits=3, output_dict=True)\n",
    "\n",
    "    # Class \"1\" corresponds to churn\n",
    "    churn_precision = report_dict[\"1\"][\"precision\"]\n",
    "    churn_recall = report_dict[\"1\"][\"recall\"]\n",
    "\n",
    "    # Business readout: how many churners caught per 100 accounts contacted?\n",
    "    # (precision * 100) = expected churners per 100 contacted\n",
    "    churners_per_100_contacted = churn_precision * 100\n",
    "\n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"roc_auc\": float(roc),\n",
    "        \"pr_auc\": float(pr),\n",
    "        \"top_pct\": float(top_pct),\n",
    "        \"threshold\": float(thr),\n",
    "        \"precision_churn_topk\": float(churn_precision),\n",
    "        \"recall_churn_topk\": float(churn_recall),\n",
    "        \"churners_per_100_contacted\": float(churners_per_100_contacted),\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'label': 'LR_VAL',\n",
       "  'roc_auc': 0.6128744669451275,\n",
       "  'pr_auc': 0.1589462560880725,\n",
       "  'top_pct': 0.2,\n",
       "  'threshold': 0.5455691240399337,\n",
       "  'precision_churn_topk': 0.18304668304668303,\n",
       "  'recall_churn_topk': 0.3378684807256236,\n",
       "  'churners_per_100_contacted': 18.304668304668304,\n",
       "  'confusion_matrix': [[2962, 665], [292, 149]]},\n",
       " {'label': 'LR_TEST',\n",
       "  'roc_auc': 0.6733395989974937,\n",
       "  'pr_auc': 0.26462185783409264,\n",
       "  'top_pct': 0.2,\n",
       "  'threshold': 0.5479689546635695,\n",
       "  'precision_churn_topk': 0.3123644251626898,\n",
       "  'recall_churn_topk': 0.42105263157894735,\n",
       "  'churners_per_100_contacted': 31.23644251626898,\n",
       "  'confusion_matrix': [[1643, 317], [198, 144]]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Champion (LR) on VAL + TEST\n",
    "\n",
    "lr_val = eval_ranked_model(best_lr, X_val, y_val, label=\"LR_VAL\", top_pct=0.20)\n",
    "lr_test = eval_ranked_model(best_lr, X_test, y_test, label=\"LR_TEST\", top_pct=0.20)\n",
    "\n",
    "lr_val, lr_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'label': 'HGB_VAL',\n",
       "  'roc_auc': 0.5856923414527101,\n",
       "  'pr_auc': 0.13083983773041796,\n",
       "  'top_pct': 0.2,\n",
       "  'threshold': 0.07006344720148443,\n",
       "  'precision_churn_topk': 0.14619164619164618,\n",
       "  'recall_churn_topk': 0.2698412698412698,\n",
       "  'churners_per_100_contacted': 14.619164619164618,\n",
       "  'confusion_matrix': [[2932, 695], [322, 119]]},\n",
       " {'label': 'HGB_TEST',\n",
       "  'roc_auc': 0.6424543501611171,\n",
       "  'pr_auc': 0.22292683064202237,\n",
       "  'top_pct': 0.2,\n",
       "  'threshold': 0.06849065846293607,\n",
       "  'precision_churn_topk': 0.23427331887201736,\n",
       "  'recall_churn_topk': 0.3157894736842105,\n",
       "  'churners_per_100_contacted': 23.427331887201735,\n",
       "  'confusion_matrix': [[1607, 353], [234, 108]]})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate Challenger (HGB) on VAL + TEST (for comparison)\n",
    "\n",
    "hgb_val = eval_ranked_model(best_hgb, X_val, y_val, label=\"HGB_VAL\", top_pct=0.20)\n",
    "hgb_test = eval_ranked_model(best_hgb, X_test, y_test, label=\"HGB_TEST\", top_pct=0.20)\n",
    "\n",
    "hgb_val, hgb_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>pr_auc</th>\n",
       "      <th>precision_churn_topk</th>\n",
       "      <th>recall_churn_topk</th>\n",
       "      <th>churners_per_100_contacted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HGB_TEST</td>\n",
       "      <td>0.642454</td>\n",
       "      <td>0.222927</td>\n",
       "      <td>0.234273</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>23.427332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HGB_VAL</td>\n",
       "      <td>0.585692</td>\n",
       "      <td>0.130840</td>\n",
       "      <td>0.146192</td>\n",
       "      <td>0.269841</td>\n",
       "      <td>14.619165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LR_TEST</td>\n",
       "      <td>0.673340</td>\n",
       "      <td>0.264622</td>\n",
       "      <td>0.312364</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>31.236443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LR_VAL</td>\n",
       "      <td>0.612874</td>\n",
       "      <td>0.158946</td>\n",
       "      <td>0.183047</td>\n",
       "      <td>0.337868</td>\n",
       "      <td>18.304668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label   roc_auc    pr_auc  precision_churn_topk  recall_churn_topk  \\\n",
       "3  HGB_TEST  0.642454  0.222927              0.234273           0.315789   \n",
       "2   HGB_VAL  0.585692  0.130840              0.146192           0.269841   \n",
       "1   LR_TEST  0.673340  0.264622              0.312364           0.421053   \n",
       "0    LR_VAL  0.612874  0.158946              0.183047           0.337868   \n",
       "\n",
       "   churners_per_100_contacted  \n",
       "3                   23.427332  \n",
       "2                   14.619165  \n",
       "1                   31.236443  \n",
       "0                   18.304668  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comparison Table (Clean Output)\n",
    "\n",
    "comparison = pd.DataFrame([lr_val, lr_test, hgb_val, hgb_test])[\n",
    "    [\"label\", \"roc_auc\", \"pr_auc\", \"precision_churn_topk\", \"recall_churn_topk\", \"churners_per_100_contacted\"]\n",
    "].sort_values(by=[\"label\"])\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_VAL : TN=2962, FP=665, FN=292, TP=149\n",
      "LR_TEST: TN=1643, FP=317, FN=198, TP=144\n",
      "HGB_VAL : TN=2932, FP=695, FN=322, TP=119\n",
      "HGB_TEST: TN=1607, FP=353, FN=234, TP=108\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrices\n",
    "\n",
    "def pretty_cm(cm):\n",
    "    tn, fp = cm[0]\n",
    "    fn, tp = cm[1]\n",
    "    return f\"TN={tn}, FP={fp}, FN={fn}, TP={tp}\"\n",
    "\n",
    "print(\"LR_VAL :\", pretty_cm(lr_val[\"confusion_matrix\"]))\n",
    "print(\"LR_TEST:\", pretty_cm(lr_test[\"confusion_matrix\"]))\n",
    "print(\"HGB_VAL :\", pretty_cm(hgb_val[\"confusion_matrix\"]))\n",
    "print(\"HGB_TEST:\", pretty_cm(hgb_test[\"confusion_matrix\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_VAL: If we contact the top 20% highest-risk accounts, we would catch about 33.8% of churners, and roughly 18.3 out of every 100 contacted accounts would actually churn (precision 18.3%).\n",
      "LR_TEST: If we contact the top 20% highest-risk accounts, we would catch about 42.1% of churners, and roughly 31.2 out of every 100 contacted accounts would actually churn (precision 31.2%).\n",
      "HGB_VAL: If we contact the top 20% highest-risk accounts, we would catch about 27.0% of churners, and roughly 14.6 out of every 100 contacted accounts would actually churn (precision 14.6%).\n",
      "HGB_TEST: If we contact the top 20% highest-risk accounts, we would catch about 31.6% of churners, and roughly 23.4 out of every 100 contacted accounts would actually churn (precision 23.4%).\n"
     ]
    }
   ],
   "source": [
    "# Business Narrative Snippet\n",
    "\n",
    "def narrative(res):\n",
    "    pct = int(res[\"top_pct\"] * 100)\n",
    "    return (\n",
    "        f\"{res['label']}: If we contact the top {pct}% highest-risk accounts, \"\n",
    "        f\"we would catch about {res['recall_churn_topk']:.1%} of churners, \"\n",
    "        f\"and roughly {res['churners_per_100_contacted']:.1f} out of every 100 contacted accounts \"\n",
    "        f\"would actually churn (precision {res['precision_churn_topk']:.1%}).\"\n",
    "    )\n",
    "\n",
    "print(narrative(lr_val))\n",
    "print(narrative(lr_test))\n",
    "print(narrative(hgb_val))\n",
    "print(narrative(hgb_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04febf9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ../reports/model_evaluation_results.json\n"
     ]
    }
   ],
   "source": [
    "# Save Evaluation Results\n",
    "\n",
    "results = {\n",
    "    \"lr_meta\": lr_meta,\n",
    "    \"hgb_meta\": hgb_meta,\n",
    "    \"evaluation\": {\n",
    "        \"lr_val\": lr_val,\n",
    "        \"lr_test\": lr_test,\n",
    "        \"hgb_val\": hgb_val,\n",
    "        \"hgb_test\": hgb_test,\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"../reports/model_evaluation_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"Saved: ../reports/model_evaluation_results.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
